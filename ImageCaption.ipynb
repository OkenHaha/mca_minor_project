{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2286d76-0fc1-4636-861d-89f5d245e1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\keith\\desktop\\dev\\mca\\mca_env\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\keith\\desktop\\dev\\mca\\mca_env\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56363fc1-018c-4aa3-a9a7-25d38aa41f1e",
   "metadata": {
    "id": "56363fc1-018c-4aa3-a9a7-25d38aa41f1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\keith\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keith\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import requests\n",
    "from math import sqrt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1dfd687-2634-4a0c-afa4-c937397488d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "d1dfd687-2634-4a0c-afa4-c937397488d9",
    "outputId": "e6973184-d8a8-467a-c146-f12a5f750f7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/Images/1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[start] a child in a pink dress is climbing up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/Images/1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[start] a girl going into a wooden building [end]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/Images/1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[start] a little girl climbing into a wooden p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/Images/1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[start] a little girl climbing the stairs to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/Images/1000268201_693b08cb0e.jpg</td>\n",
       "      <td>[start] a little girl in a pink dress going in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     image  \\\n",
       "0  ./data/Images/1000268201_693b08cb0e.jpg   \n",
       "1  ./data/Images/1000268201_693b08cb0e.jpg   \n",
       "2  ./data/Images/1000268201_693b08cb0e.jpg   \n",
       "3  ./data/Images/1000268201_693b08cb0e.jpg   \n",
       "4  ./data/Images/1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  [start] a child in a pink dress is climbing up...  \n",
       "1  [start] a girl going into a wooden building [end]  \n",
       "2  [start] a little girl climbing into a wooden p...  \n",
       "3  [start] a little girl climbing the stairs to h...  \n",
       "4  [start] a little girl in a pink dress going in...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions = pd.read_csv('./data/captions.txt')\n",
    "captions['image'] = captions['image'].apply(lambda x: f'./data/Images/{x}')\n",
    "captions.head()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = '[start] ' + text + ' [end]'\n",
    "    return text\n",
    "\n",
    "\n",
    "captions['caption'] = captions['caption'].apply(preprocess)\n",
    "captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7618d3-3b03-474e-898e-20ad34019ac0",
   "metadata": {
    "id": "6f7618d3-3b03-474e-898e-20ad34019ac0"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "VOCABULARY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "EMBEDDING_DIM = 512\n",
    "UNITS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24148c9d-fb76-4ebf-9d2e-8e8d10252a02",
   "metadata": {
    "id": "24148c9d-fb76-4ebf-9d2e-8e8d10252a02",
    "outputId": "990cf391-ed7d-4011-b328-56a55ae3bd10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\keith\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\keith\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCABULARY_SIZE,\n",
    "    standardize=None,\n",
    "    output_sequence_length=MAX_LENGTH)\n",
    "\n",
    "tokenizer.adapt(captions['caption'])\n",
    "\n",
    "word2idx = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "\n",
    "idx2word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2cf706a-f92b-4496-8f1c-4b72faa83b2b",
   "metadata": {
    "id": "c2cf706a-f92b-4496-8f1c-4b72faa83b2b"
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(captions['image'], captions['caption']):\n",
    "    img_to_cap_vector[img].append(cap)\n",
    "\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index],\n",
    "                                          img_keys[slice_index:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70acc18-b3dd-4ac2-94e1-5cf1335b9bbf",
   "metadata": {
    "id": "c70acc18-b3dd-4ac2-94e1-5cf1335b9bbf"
   },
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "train_captions = []\n",
    "for imgt in img_name_train_keys:\n",
    "    capt_len = len(img_to_cap_vector[imgt])\n",
    "    train_imgs.extend([imgt] * capt_len)\n",
    "    train_captions.extend(img_to_cap_vector[imgt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f6ee420-40a7-4806-acc0-8ae61e48b0c5",
   "metadata": {
    "id": "4f6ee420-40a7-4806-acc0-8ae61e48b0c5"
   },
   "outputs": [],
   "source": [
    "val_imgs = []\n",
    "val_captions = []\n",
    "for imgv in img_name_val_keys:\n",
    "    capv_len = len(img_to_cap_vector[imgv])\n",
    "    val_imgs.extend([imgv] * capv_len)\n",
    "    val_captions.extend(img_to_cap_vector[imgv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a12ab53-f581-45e3-8711-dd79e95338e5",
   "metadata": {
    "id": "4a12ab53-f581-45e3-8711-dd79e95338e5",
    "outputId": "9cf3b835-30e2-45df-e6e0-01d33b7233b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32360, 32360, 8095, 8095)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec8d9f3-067a-4018-9448-9b9859ae788f",
   "metadata": {
    "id": "5ec8d9f3-067a-4018-9448-9b9859ae788f"
   },
   "outputs": [],
   "source": [
    "def load_data(img_path, caption):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = img / 255.\n",
    "    caption = tokenizer(caption)\n",
    "    return img, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7e01632-5fd6-4a4b-a80f-50c62248f478",
   "metadata": {
    "id": "e7e01632-5fd6-4a4b-a80f-50c62248f478"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_imgs, train_captions))\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_imgs, val_captions))\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "image_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5470adf3-6ee1-4181-a292-de1e6801771b",
   "metadata": {
    "id": "5470adf3-6ee1-4181-a292-de1e6801771b"
   },
   "outputs": [],
   "source": [
    "def CNN_Encoder():\n",
    "    inception_v3 = tf.keras.applications.InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    inception_v3.trainable = False\n",
    "\n",
    "    output = inception_v3.output\n",
    "    output = tf.keras.layers.Reshape(\n",
    "        (-1, output.shape[-1]))(output)\n",
    "\n",
    "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21037f15-b390-4ee5-995d-7f10a58650a6",
   "metadata": {
    "id": "21037f15-b390-4ee5-995d-7f10a58650a6"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "\n",
    "    def call(self, x, training):\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        attn_output = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=None,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        x = self.layer_norm_2(x + attn_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32126adc-7533-4f67-808e-bd9fdebc829b",
   "metadata": {
    "id": "32126adc-7533-4f67-808e-bd9fdebc829b"
   },
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            vocab_size, embed_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            max_len, embed_dim, input_shape=(None, max_len))\n",
    "\n",
    "\n",
    "    def call(self, input_ids):\n",
    "        length = tf.shape(input_ids)[-1]\n",
    "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
    "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
    "\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81e45262-c274-49be-8b39-75cf4f1e7965",
   "metadata": {
    "id": "81e45262-c274-49be-8b39-75cf4f1e7965",
    "outputId": "7630de11-b3d6-4833-95be-f248ebcc4af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 40, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embeddings(tokenizer.vocabulary_size(), EMBEDDING_DIM, MAX_LENGTH)(next(iter(train_dataset))[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e268155-8533-4073-b660-4d5bd0c8caf4",
   "metadata": {
    "id": "7e268155-8533-4073-b660-4d5bd0c8caf4"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, units, num_heads):\n",
    "        super().__init__()\n",
    "        self.embedding = Embeddings(\n",
    "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
    "\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
    "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def call(self, input_ids, encoder_output, training, mask=None):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "\n",
    "        combined_mask = None\n",
    "        padding_mask = None\n",
    "\n",
    "        if mask is not None:\n",
    "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attn_output_1 = self.attention_1(\n",
    "            query=embeddings,\n",
    "            value=embeddings,\n",
    "            key=embeddings,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
    "\n",
    "        attn_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_output,\n",
    "            key=encoder_output,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training\n",
    "        )\n",
    "\n",
    "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "039fcc83-aef1-4ee3-9c7b-7dd30ea13cd6",
   "metadata": {
    "id": "039fcc83-aef1-4ee3-9c7b-7dd30ea13cd6"
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(tf.keras.Model):\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        \n",
    "        imgs, captions = inputs\n",
    "        \n",
    "        if self.image_aug and training:\n",
    "            imgs = self.image_aug(imgs)\n",
    "            \n",
    "        img_embed = self.cnn_model(imgs)\n",
    "        encoder_output = self.encoder(img_embed)\n",
    "        \n",
    "        y_pred = self.decoder(captions, encoder_output, \n",
    "                               training=training, mask=mask)\n",
    "        \n",
    "        return y_pred\n",
    "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.image_aug = image_aug\n",
    "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")\n",
    "\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
    "        encoder_output = self.encoder(img_embed, training=True)\n",
    "        y_input = captions[:, :-1]\n",
    "        y_true = captions[:, 1:]\n",
    "        mask = (y_true != 0)\n",
    "        y_pred = self.decoder(\n",
    "            y_input, encoder_output, training=True, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(y_true, y_pred, mask)\n",
    "        acc = self.calculate_accuracy(y_true, y_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        if self.image_aug:\n",
    "            imgs = self.image_aug(imgs)\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, acc = self.compute_loss_and_acc(\n",
    "                img_embed, captions\n",
    "            )\n",
    "\n",
    "        train_vars = (\n",
    "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        )\n",
    "        grads = tape.gradient(loss, train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        imgs, captions = batch\n",
    "\n",
    "        img_embed = self.cnn_model(imgs)\n",
    "\n",
    "        loss, acc = self.compute_loss_and_acc(\n",
    "            img_embed, captions, training=False\n",
    "        )\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.acc_tracker.update_state(acc)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0790208-30a9-46ef-8bfb-ab139a6f9b01",
   "metadata": {
    "id": "e0790208-30a9-46ef-8bfb-ab139a6f9b01",
    "outputId": "dea97e42-10ea-487b-c642-6d5a1e2cf2cc"
   },
   "outputs": [],
   "source": [
    "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
    "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
    "\n",
    "cnn_model = CNN_Encoder()\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
    ")\n",
    "\n",
    "\n",
    "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "\n",
    "caption_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=cross_entropy\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc7df9-a80e-484a-94e2-5be04ea329e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522d812-da9e-4069-af5f-e28cae5ebd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "003b9124-045f-438d-8cbb-5e45f78c22fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m caption_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, MAX_LENGTH)  \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcaption_model\u001b[49m\u001b[38;5;241m.\u001b[39mbuild([img_shape, caption_shape])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caption_model' is not defined"
     ]
    }
   ],
   "source": [
    "img_shape = (None, 299, 299, 3)\n",
    "caption_shape = (None, MAX_LENGTH)  \n",
    "\n",
    "# Build model\n",
    "caption_model.build([img_shape, caption_shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aab86b2e-18d9-4bdf-b784-3ba7b580c80e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m dummy_captions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m40\u001b[39m))\n\u001b[0;32m      8\u001b[0m _ \u001b[38;5;241m=\u001b[39m caption_model((dummy_imgs, dummy_captions))\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcaption_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_captioning_transformer_weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\Desktop\\dev\\MCA\\mca_env\\lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "img_shape = (None, 299, 299, 3)\n",
    "caption_shape = (None, MAX_LENGTH)  \n",
    "\n",
    "# Build model\n",
    "caption_model.build([img_shape, caption_shape])\n",
    "dummy_imgs = np.zeros((32, 299, 299, 3))  \n",
    "dummy_captions = np.zeros((32, 40))\n",
    "_ = caption_model((dummy_imgs, dummy_captions))\n",
    "caption_model.load_weights('image_captioning_transformer_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "357aba32-9445-429b-927d-2c4cabc90f5c",
   "metadata": {
    "id": "357aba32-9445-429b-927d-2c4cabc90f5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1012/1012 [==============================] - 9809s 10s/step - loss: 4.0878 - acc: 0.2489 - val_loss: 3.7025 - val_acc: 0.3329\n",
      "Epoch 2/5\n",
      "1012/1012 [==============================] - 9656s 10s/step - loss: 3.4182 - acc: 0.3464 - val_loss: 3.4694 - val_acc: 0.3555\n",
      "Epoch 3/5\n",
      "1012/1012 [==============================] - 14961s 15s/step - loss: 3.1907 - acc: 0.3685 - val_loss: 3.4037 - val_acc: 0.3614\n",
      "Epoch 4/5\n",
      "1012/1012 [==============================] - 9118s 9s/step - loss: 3.0465 - acc: 0.3820 - val_loss: 3.3618 - val_acc: 0.3676\n",
      "Epoch 5/5\n",
      "1012/1012 [==============================] - 9074s 9s/step - loss: 2.9305 - acc: 0.3932 - val_loss: 3.3238 - val_acc: 0.3742\n"
     ]
    }
   ],
   "source": [
    "history = caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "903a55a0-258c-40b3-bf88-0ab40dc85004",
   "metadata": {
    "id": "903a55a0-258c-40b3-bf88-0ab40dc85004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word(2).numpy().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e70e967f-8645-4b31-9196-ea34d202a3f5",
   "metadata": {
    "id": "e70e967f-8645-4b31-9196-ea34d202a3f5"
   },
   "outputs": [],
   "source": [
    "def load_image_from_path(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = img / 255.\n",
    "    return img\n",
    "\n",
    "\n",
    "def generate_caption(img_path):\n",
    "    img = load_image_from_path(img_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img_embed = caption_model.cnn_model(img)\n",
    "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
    "\n",
    "    y_inp = '[start]'\n",
    "    for i in range(MAX_LENGTH-1):\n",
    "        tokenized = tokenizer([y_inp])[:, :-1]\n",
    "        mask = tf.cast(tokenized != 0, tf.int32)\n",
    "        pred = caption_model.decoder(\n",
    "            tokenized, img_encoded, training=False, mask=mask)\n",
    "\n",
    "        pred_idx = np.argmax(pred[0, i, :])\n",
    "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
    "        if pred_word == '[end]':\n",
    "            break\n",
    "\n",
    "        y_inp += ' ' + pred_word\n",
    "\n",
    "    y_inp = y_inp.replace('[start] ', '')\n",
    "    return y_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601659da-e78d-483c-9d1e-1a661e2d26f3",
   "metadata": {
    "id": "601659da-e78d-483c-9d1e-1a661e2d26f3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(val_imgs))\n\u001b[0;32m      2\u001b[0m img_path \u001b[38;5;241m=\u001b[39m val_imgs[idx]\n\u001b[1;32m----> 4\u001b[0m pred_caption \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Caption:\u001b[39m\u001b[38;5;124m'\u001b[39m, pred_caption)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mgenerate_caption\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m load_image_from_path(img_path)\n\u001b[0;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m img_embed \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_model\u001b[49m\u001b[38;5;241m.\u001b[39mcnn_model(img)\n\u001b[0;32m     13\u001b[0m img_encoded \u001b[38;5;241m=\u001b[39m caption_model\u001b[38;5;241m.\u001b[39mencoder(img_embed, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m y_inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[start]\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caption_model' is not defined"
     ]
    }
   ],
   "source": [
    "idx = random.randrange(0, len(val_imgs))\n",
    "img_path = val_imgs[idx]\n",
    "\n",
    "pred_caption = generate_caption(img_path)\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dce4e02-a13c-45fc-9f7a-554bc3cbcc2d",
   "metadata": {
    "id": "7dce4e02-a13c-45fc-9f7a-554bc3cbcc2d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.princeton.edu/sites/default/files/styles/1x_full_2x_half_crop/public/images/2022/02/KOA_Nassau_2697x1517.jpg?itok=Bg2K7j7J\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m      3\u001b[0m im\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m pred_caption \u001b[38;5;241m=\u001b[39m generate_caption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://www.princeton.edu/sites/default/files/styles/1x_full_2x_half_crop/public/images/2022/02/KOA_Nassau_2697x1517.jpg?itok=Bg2K7j7J\"\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "im.save('tmp.jpg')\n",
    "\n",
    "pred_caption = generate_caption('tmp.jpg')\n",
    "print('Predicted Caption:', pred_caption)\n",
    "print()\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303fafa7-77a7-4d35-8ab9-76517179b421",
   "metadata": {
    "id": "303fafa7-77a7-4d35-8ab9-76517179b421"
   },
   "outputs": [],
   "source": [
    "#caption_model.save_weights('image_captioning.h5')\n",
    "with open('image_captioning_transformer_weights.h5', 'wb') as f:\n",
    "    f.write(open('image_captioning_transformer_weights.h5', 'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5bdf1-c447-43d4-9185-9f6be0c662bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
